{"cells": [{"cell_type": "code", "metadata": {}, "source": ["from fastcore.utils import *\n", "import fastcore.all as fc, re, math, itertools, functools, numpy as np, types, typing, dataclasses, matplotlib.pyplot as plt, collections, regex\n", "from regex import search\n", "from collections import Counter\n", "from collections.abc import Iterable\n", "np.set_printoptions(linewidth=150, suppress=True)\n", "plt.rcParams['figure.dpi'] = 50"], "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": ["!ls static"], "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Understanding-ISO-17025-for-Laser-Power-and-Energy-Measurement-043520ed-a855-43f6-b05c-a9c83afded79.pdf\r\n", "enhanced_laser_nodes.pkl\r\n", "labmax-touch-ds.pdf\r\n", "laser_nodes.db\r\n", "laser_qdrant.pkl\r\n", "multipurpose-energymax-sensors-usb-rs232-ds.pdf\r\n", "parsed_lmc_docs.pkl\r\n", "pm10k-plus-ds.pdf\r\n", "requirements.txt\r\n"]}], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": ["from fastcore.utils import *\n", "path = Path('static')"], "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": ["!pip install llama-cloud-services llama-index-core llama-index-readers-file llama-index-embeddings-openai llama-index-llms-openai llama_index.postprocessor.cohere_rerank llama-index-program-openai qdrant-client llama-index-vector-stores-qdrant python-fasthtml \"fastapi[standard]\" MonsterUI"], "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Defaulting to user installation because normal site-packages is not writeable\r\n", "Requirement already satisfied: llama-cloud-services in ./.local/lib/python3.11/site-packages (0.6.0)\r\n", "Requirement already satisfied: llama-index-core in ./.local/lib/python3.11/site-packages (0.12.25)\r\n", "Requirement already satisfied: llama-index-readers-file in ./.local/lib/python3.11/site-packages (0.4.4)\r\n", "Requirement already satisfied: llama-index-embeddings-openai in ./.local/lib/python3.11/site-packages (0.3.1)\r\n", "Requirement already satisfied: llama-index-llms-openai in ./.local/lib/python3.11/site-packages (0.3.27)\r\n", "Requirement already satisfied: llama_index.postprocessor.cohere_rerank in ./.local/lib/python3.11/site-packages (0.3.0)\r\n", "Requirement already satisfied: llama-index-program-openai in ./.local/lib/python3.11/site-packages (0.3.1)\r\n", "Requirement already satisfied: qdrant-client in ./.local/lib/python3.11/site-packages (1.13.3)\r\n", "Requirement already satisfied: llama-index-vector-stores-qdrant in ./.local/lib/python3.11/site-packages (0.6.0)\r\n", "Requirement already satisfied: python-fasthtml in /usr/local/lib/python3.11/site-packages (0.12.5)\r\n", "Requirement already satisfied: MonsterUI in /usr/local/lib/python3.11/site-packages (1.0.11)\r\n", "Requirement already satisfied: fastapi[standard] in ./.local/lib/python3.11/site-packages (0.115.12)\r\n", "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.11/site-packages (from llama-cloud-services) (8.1.8)\r\n", "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.11 in ./.local/lib/python3.11/site-packages (from llama-cloud-services) (0.1.12)\r\n", "Requirement already satisfied: pydantic!=2.10 in /usr/local/lib/python3.11/site-packages (from llama-cloud-services) (2.10.6)\r\n", "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/site-packages (from llama-cloud-services) (1.1.0)\r\n", "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/site-packages (from llama-index-core) (6.0.2)\r\n", "Requirement already satisfied: SQLAlchemy>=1.4.49 in ./.local/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.38)\r\n", "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/site-packages (from llama-index-core) (3.11.14)\r\n", "Requirement already satisfied: dataclasses-json in ./.local/lib/python3.11/site-packages (from llama-index-core) (0.6.7)\r\n", "Requirement already satisfied: deprecated>=1.2.9.3 in ./.local/lib/python3.11/site-packages (from llama-index-core) (1.2.18)\r\n", "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in ./.local/lib/python3.11/site-packages (from llama-index-core) (1.0.8)\r\n", "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in ./.local/lib/python3.11/site-packages (from llama-index-core) (1.2.0)\r\n", "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/site-packages (from llama-index-core) (2024.6.1)\r\n", "Requirement already satisfied: httpx in /usr/local/lib/python3.11/site-packages (from llama-index-core) (0.28.1)\r\n", "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in ./.local/lib/python3.11/site-packages (from llama-index-core) (1.6.0)\r\n", "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/site-packages (from llama-index-core) (3.3)\r\n", "Requirement already satisfied: nltk>3.8.1 in ./.local/lib/python3.11/site-packages (from llama-index-core) (3.9.1)\r\n", "Requirement already satisfied: numpy in /usr/local/lib/python3.11/site-packages (from llama-index-core) (2.1.2)\r\n", "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/site-packages (from llama-index-core) (11.0.0)\r\n", "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/site-packages (from llama-index-core) (2.32.3)\r\n", "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/site-packages (from llama-index-core) (9.0.0)\r\n", "Requirement already satisfied: tiktoken>=0.3.3 in ./.local/lib/python3.11/site-packages (from llama-index-core) (0.8.0)\r\n", "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/site-packages (from llama-index-core) (4.67.1)\r\n", "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/site-packages (from llama-index-core) (4.12.2)\r\n", "Requirement already satisfied: typing-inspect>=0.8.0 in ./.local/lib/python3.11/site-packages (from llama-index-core) (0.9.0)\r\n", "Requirement already satisfied: wrapt in ./.local/lib/python3.11/site-packages (from llama-index-core) (1.17.2)\r\n", "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/site-packages (from llama-index-readers-file) (4.13.3)\r\n", "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (from llama-index-readers-file) (2.2.3)\r\n", "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in ./.local/lib/python3.11/site-packages (from llama-index-readers-file) (5.1.0)\r\n", "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in ./.local/lib/python3.11/site-packages (from llama-index-readers-file) (0.0.26)\r\n", "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.11/site-packages (from llama-index-embeddings-openai) (1.68.2)\r\n", "Requirement already satisfied: cohere<6.0.0,>=5.1.1 in ./.local/lib/python3.11/site-packages (from llama_index.postprocessor.cohere_rerank) (5.14.0)\r\n", "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in ./.local/lib/python3.11/site-packages (from llama-index-program-openai) (0.4.6)\r\n", "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.11/site-packages (from qdrant-client) (1.71.0)\r\n", "Requirement already satisfied: grpcio-tools>=1.41.0 in ./.local/lib/python3.11/site-packages (from qdrant-client) (1.71.0)\r\n", "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in ./.local/lib/python3.11/site-packages (from qdrant-client) (2.10.1)\r\n", "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.11/site-packages (from qdrant-client) (2.3.0)\r\n", "Requirement already satisfied: fastcore>=1.7.18 in /usr/local/lib/python3.11/site-packages (from python-fasthtml) (1.8.0)\r\n", "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/site-packages (from python-fasthtml) (2.9.0.post0)\r\n", "Requirement already satisfied: starlette>0.33 in /usr/local/lib/python3.11/site-packages (from python-fasthtml) (0.46.1)\r\n", "Requirement already satisfied: oauthlib in /usr/local/lib/python3.11/site-packages (from python-fasthtml) (3.2.2)\r\n", "Requirement already satisfied: itsdangerous in /usr/local/lib/python3.11/site-packages (from python-fasthtml) (2.2.0)\r\n", "Requirement already satisfied: uvicorn>=0.30 in /usr/local/lib/python3.11/site-packages (from uvicorn[standard]>=0.30->python-fasthtml) (0.34.0)\r\n", "Requirement already satisfied: fastlite>=0.1.1 in /usr/local/lib/python3.11/site-packages (from python-fasthtml) (0.1.2)\r\n", "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/site-packages (from python-fasthtml) (0.0.20)\r\n", "Requirement already satisfied: fastapi-cli>=0.0.5 in ./.local/lib/python3.11/site-packages (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (0.0.7)\r\n", "Requirement already satisfied: jinja2>=3.1.5 in ./.local/lib/python3.11/site-packages (from fastapi[standard]) (3.1.6)\r\n", "Requirement already satisfied: email-validator>=2.0.0 in ./.local/lib/python3.11/site-packages (from fastapi[standard]) (2.2.0)\r\n", "Requirement already satisfied: lxml in /usr/local/lib/python3.11/site-packages (from MonsterUI) (5.3.1)\r\n", "Requirement already satisfied: mistletoe in /usr/local/lib/python3.11/site-packages (from MonsterUI) (1.4.0)\r\n", "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (2.6.1)\r\n", "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.2)\r\n", "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (25.3.0)\r\n", "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.5.0)\r\n", "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.2.0)\r\n", "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (0.3.1)\r\n", "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.18.3)\r\n", "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.6)\r\n", "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in ./.local/lib/python3.11/site-packages (from cohere<6.0.0,>=5.1.1->llama_index.postprocessor.cohere_rerank) (1.10.0)\r\n", "Requirement already satisfied: httpx-sse==0.4.0 in ./.local/lib/python3.11/site-packages (from cohere<6.0.0,>=5.1.1->llama_index.postprocessor.cohere_rerank) (0.4.0)\r\n", "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/site-packages (from cohere<6.0.0,>=5.1.1->llama_index.postprocessor.cohere_rerank) (2.27.2)\r\n", "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.11/site-packages (from cohere<6.0.0,>=5.1.1->llama_index.postprocessor.cohere_rerank) (0.21.1)\r\n", "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in ./.local/lib/python3.11/site-packages (from cohere<6.0.0,>=5.1.1->llama_index.postprocessor.cohere_rerank) (2.32.0.20250306)\r\n", "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/site-packages (from email-validator>=2.0.0->fastapi[standard]) (2.7.0)\r\n", "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.11/site-packages (from email-validator>=2.0.0->fastapi[standard]) (3.10)\r\n", "Requirement already satisfied: typer>=0.12.3 in ./.local/lib/python3.11/site-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (0.12.5)\r\n", "Requirement already satisfied: rich-toolkit>=0.11.1 in ./.local/lib/python3.11/site-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (0.14.1)\r\n", "Requirement already satisfied: packaging in /usr/local/lib/python3.11/site-packages (from fastcore>=1.7.18->python-fasthtml) (24.2)\r\n", "Requirement already satisfied: apswutils>=0.0.2 in /usr/local/lib/python3.11/site-packages (from fastlite>=0.1.1->python-fasthtml) (0.0.2)\r\n", "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /usr/local/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant-client) (5.29.4)\r\n", "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant-client) (65.5.1)\r\n", "Requirement already satisfied: anyio in /usr/local/lib/python3.11/site-packages (from httpx->llama-index-core) (4.9.0)\r\n", "Requirement already satisfied: certifi in /usr/local/lib/python3.11/site-packages (from httpx->llama-index-core) (2025.1.31)\r\n", "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/site-packages (from httpx->llama-index-core) (1.0.7)\r\n", "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\r\n", "Requirement already satisfied: h2<5,>=3 in ./.local/lib/python3.11/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\r\n", "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2>=3.1.5->fastapi[standard]) (2.1.5)\r\n", "Requirement already satisfied: joblib in ./.local/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core) (1.4.2)\r\n", "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core) (2024.11.6)\r\n", "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.9.0)\r\n", "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (0.9.0)\r\n", "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.3.1)\r\n", "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic!=2.10->llama-cloud-services) (0.7.0)\r\n", "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core) (3.4.1)\r\n", "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.1.1)\r\n", "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.local/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core) (1.0.0)\r\n", "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/site-packages (from uvicorn[standard]>=0.30->python-fasthtml) (0.6.4)\r\n", "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/site-packages (from uvicorn[standard]>=0.30->python-fasthtml) (0.21.0)\r\n", "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/site-packages (from uvicorn[standard]>=0.30->python-fasthtml) (1.0.4)\r\n", "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/site-packages (from uvicorn[standard]>=0.30->python-fasthtml) (13.1)\r\n", "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.local/lib/python3.11/site-packages (from dataclasses-json->llama-index-core) (3.26.1)\r\n", "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas->llama-index-readers-file) (2025.2)\r\n", "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas->llama-index-readers-file) (2025.2)\r\n", "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil->python-fasthtml) (1.17.0)\r\n", "Requirement already satisfied: apsw in /usr/local/lib/python3.11/site-packages (from apswutils>=0.0.2->fastlite>=0.1.1->python-fasthtml) (3.49.1.0)\r\n", "Requirement already satisfied: hyperframe<7,>=6.1 in ./.local/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\r\n", "Requirement already satisfied: hpack<5,>=4.1 in ./.local/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\r\n", "Requirement already satisfied: rich>=13.7.1 in ./.local/lib/python3.11/site-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (13.9.4)\r\n", "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/site-packages (from tokenizers<1,>=0.15->cohere<6.0.0,>=5.1.1->llama_index.postprocessor.cohere_rerank) (0.29.3)\r\n", "Requirement already satisfied: shellingham>=1.3.0 in ./.local/lib/python3.11/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (1.5.4)\r\n", "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0.0,>=5.1.1->llama_index.postprocessor.cohere_rerank) (3.13.1)\r\n", "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.local/lib/python3.11/site-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (3.0.0)\r\n", "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/site-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (2.19.1)\r\n", "Requirement already satisfied: mdurl~=0.1 in ./.local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]) (0.1.2)\r\n", "\r\n", "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\r\n", "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"]}], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": ["import asyncio, threading\n", "\n", "def run_in_thread(coro):\n", "    res = None\n", "    def target():\n", "        nonlocal res\n", "        asyncio.set_event_loop_policy(asyncio.DefaultEventLoopPolicy())\n", "        loop = asyncio.new_event_loop()\n", "        res = loop.run_until_complete(coro)\n", "        loop.close()\n", "    t = threading.Thread(target=target)\n", "    t.start(); t.join()\n", "    return res\n", "\n", "# usage of function\n", "# variable = run_in_thread(<pass a function>)"], "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": ["import os\n", "import pickle\n", "import sqlite3\n", "from typing import List\n", "from llama_index.core.schema import NodeWithScore, TextNode, QueryBundle\n", "from llama_index.core import StorageContext, VectorStoreIndex\n", "from llama_index.core.retrievers import BaseRetriever\n", "from llama_index.core.memory import ChatMemoryBuffer\n", "from llama_index.core.chat_engine import ContextChatEngine\n", "from qdrant_client import QdrantClient\n", "from llama_index.vector_stores.qdrant import QdrantVectorStore\n", "from llama_index.postprocessor.cohere_rerank import CohereRerank\n", "from llama_index.llms.openai import OpenAI\n", "from llama_index.embeddings.openai import OpenAIEmbedding\n", "from llama_index.core import Settings\n", "\n", "# Initialize LLM and embeddings\n", "llm = OpenAI(model=\"gpt-4o\", api_key=os.environ[\"OPENAI_API_KEY\"])\n", "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n", "Settings.llm = llm\n", "Settings.embed_model = embed_model\n", "\n", "# SQLite FTS Retriever\n", "class SQLiteFTSRetriever:\n", "    def __init__(self, db_path=\"static/laser_nodes.db\", top_k=5):\n", "        self.db_path = db_path\n", "        self.top_k = top_k\n", "        \n", "    def retrieve(self, query_str: str) -> List[NodeWithScore]:\n", "        conn = sqlite3.connect(self.db_path)\n", "        c = conn.cursor()\n", "        escaped_query = f'\"{query_str}\"'\n", "        c.execute(\n", "            f\"\"\"\n", "            SELECT nodes.node_id, nodes.content, nodes.metadata, nodes_fts.rank\n", "            FROM nodes_fts \n", "            JOIN nodes ON nodes_fts.rowid = nodes.rowid\n", "            WHERE nodes_fts MATCH ? \n", "            ORDER BY nodes_fts.rank\n", "            LIMIT {self.top_k}\n", "            \"\"\", \n", "            (escaped_query,)\n", "        )\n", "        \n", "        results = []\n", "        for node_id, content, metadata_blob, rank in c.fetchall():\n", "            import json\n", "            metadata = json.loads(metadata_blob)\n", "            node = TextNode(text=content, metadata=metadata, id_=node_id)\n", "            score = 1.0 / (1.0 + float(rank))\n", "            results.append(NodeWithScore(node=node, score=score))\n", "        \n", "        conn.close()\n", "        return results\n", "\n", "# Hybrid Retriever with Reranking\n", "class HybridRetrieverWithReranking(BaseRetriever):\n", "    def __init__(self, vector_retriever, keyword_retriever, reranker, \n", "                vector_weight=0.7, keyword_weight=0.3, initial_top_k=20):\n", "        self.vector_retriever = vector_retriever\n", "        self.keyword_retriever = keyword_retriever\n", "        self.reranker = reranker\n", "        self.vector_weight = vector_weight\n", "        self.keyword_weight = keyword_weight\n", "        self.initial_top_k = initial_top_k\n", "        super().__init__()\n", "    \n", "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n", "        query_str = query_bundle.query_str\n", "        \n", "        # Get results from both retrievers\n", "        vector_results = self.vector_retriever.retrieve(query_str)\n", "        keyword_results = self.keyword_retriever.retrieve(query_str)\n", "        \n", "        # Combine scores\n", "        node_scores = {}\n", "        for i, result in enumerate(vector_results):\n", "            node_id = result.node.node_id\n", "            score = self.vector_weight * (1.0 / (i + 1))\n", "            node_scores[node_id] = {\"node\": result.node, \"score\": score}\n", "        \n", "        for i, result in enumerate(keyword_results):\n", "            node_id = result.node.node_id\n", "            keyword_score = self.keyword_weight * (1.0 / (i + 1))\n", "            if node_id in node_scores:\n", "                node_scores[node_id][\"score\"] += keyword_score\n", "            else:\n", "                node_scores[node_id] = {\"node\": result.node, \"score\": keyword_score}\n", "        \n", "        # Sort by score\n", "        sorted_results = sorted(\n", "            node_scores.values(), \n", "            key=lambda x: x[\"score\"], \n", "            reverse=True\n", "        )\n", "        \n", "        # Convert to NodeWithScore objects\n", "        initial_results = [\n", "            NodeWithScore(node=item[\"node\"], score=item[\"score\"]) \n", "            for item in sorted_results[:self.initial_top_k]\n", "        ]\n", "        \n", "        # Apply reranking\n", "        reranked_nodes = self.reranker.postprocess_nodes(\n", "            initial_results, \n", "            query_bundle\n", "        )\n", "        \n", "        return reranked_nodes\n", "\n", "# Load function for vector stores and create retriever\n", "def create_retriever(cohere_api_key):\n", "    # Load SQLite retriever\n", "    sqlite_retriever = SQLiteFTSRetriever()\n", "    \n", "    # Load Qdrant collection\n", "    with open('static/laser_qdrant.pkl', 'rb') as f:\n", "        collection_data = pickle.load(f)\n", "    \n", "    # Create Qdrant client and collection\n", "    qdrant_client = QdrantClient(\":memory:\")\n", "    collection_name = collection_data[\"collection_name\"]\n", "    \n", "    if qdrant_client.collection_exists(collection_name):\n", "        qdrant_client.delete_collection(collection_name)\n", "    \n", "    qdrant_client.create_collection(\n", "        collection_name=collection_name,\n", "        vectors_config={\"size\": collection_data[\"vector_size\"], \"distance\": \"Cosine\"}\n", "    )\n", "    \n", "    if collection_data[\"points\"]:\n", "        qdrant_client.upsert(\n", "            collection_name=collection_name,\n", "            points=collection_data[\"points\"]\n", "        )\n", "    \n", "    # Create vector store and retriever\n", "    vector_store = QdrantVectorStore(\n", "        client=qdrant_client,\n", "        collection_name=collection_name\n", "    )\n", "    \n", "    index = VectorStoreIndex.from_vector_store(vector_store)\n", "    vector_retriever = index.as_retriever(similarity_top_k=10)\n", "    \n", "    # Create Cohere reranker\n", "    reranker = CohereRerank(\n", "        api_key=cohere_api_key,\n", "        model=\"rerank-v3.5\",\n", "        top_n=5\n", "    )\n", "    \n", "    # Create and return hybrid retriever with reranking\n", "    return HybridRetrieverWithReranking(\n", "        vector_retriever=vector_retriever,\n", "        keyword_retriever=sqlite_retriever,\n", "        reranker=reranker,\n", "        vector_weight=0.7,\n", "        keyword_weight=0.3,\n", "        initial_top_k=20\n", "    )\n", "\n", "# Create chat engine\n", "def create_chat_engine(retriever):\n", "    memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n", "    return ContextChatEngine.from_defaults(\n", "        retriever=retriever,\n", "        memory=memory,\n", "        system_prompt=\"\"\"You are a helpful assistant specializing in laser measurement technology.\n", "        Answer questions based on the context provided. If you don't know the answer, say so.\"\"\"\n", "    )\n", "\n", "# Main initialization function to use in FastAPI\n", "def init_chat_engine():\n", "    cohere_api_key = os.environ.get(\"COHERE_API_KEY\")\n", "    retriever = create_retriever(cohere_api_key)\n", "    return create_chat_engine(retriever)\n", "\n", "# This is where you would start building your FastAPI app\n", "# Example:\n", "# chat_engine = init_chat_engine()"], "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": ["# Test the refactored code\n", "if __name__ == \"__main__\":\n", "    # Initialize the chat engine\n", "    chat_engine = init_chat_engine()\n", "    \n", "    # Test with a simple query\n", "    test_query = \"What are the specifications of the PM10K+ laser power sensor?\"\n", "    response = chat_engine.chat(test_query)\n", "    \n", "    print(f\"Query: {test_query}\")\n", "    print(f\"Response: {response.response}\")"], "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Query: What are the specifications of the PM10K+ laser power sensor?\n", "Response: The specifications of the PM10K+ laser power sensor are as follows:\n", "\n", "- **Wavelength Range:** 0.19 to 11 \u00b5m\n", "- **Power Range:** 100 to 10,000 W\n", "- **Maximum Intermittent Power (<5 min.):** 12 kW (beam size dependent)\n", "- **Noise Equivalent Power (at 6 lpm):**\n", "  - USB and RS-232: <0.2 W\n", "  - DB-25 with meter: <1.0 W\n", "- **Maximum Power Density (kW/cm\u00b2):**\n", "  - 6.0 at 1 kW\n", "  - 2.6 at 5 kW\n", "  - 2.7 at 10 kW\n", "  - 2.5 at 12 kW\n", "- **Recommended Minimum Beam Size (mm):**\n", "  - 6.5 at 1 kW\n", "  - 22 at 5 kW\n", "  - 31 at 10 kW\n", "  - 35 at 12 kW\n", "- **Minimum Water Flow Rate:** 6 lpm (~10 PSI)\n", "- **Water Temperature Range:** 10 to 25 \u00b0C\n", "- **Response Time (to 0 to 99%) (at 6 lpm):**\n", "  - Speed-up On: <3 seconds\n", "  - Speed-up Off: 10 seconds\n", "- **Maximum Energy Density (1064 nm, 10 ns):** 600 mJ/cm\u00b2\n", "- **Detector Coating:** BB+\n", "- **Detector Dimensions:** 65 x 65 mm\n", "- **Calibration Uncertainty:** \u00b13%\n", "- **Power Linearity:** \u00b12%\n", "- **Spectral Compensation Accuracy:** \u00b11.5%\n", "- **Calibration Wavelength:** 1070 nm\n", "- **Cooling Method:** Water\n", "- **Interface Type:** DB-25 + USB, RS-232\n", "- **Cable Length:** 2.0 m (DB-25)\n", "- **Part Numbers:**\n", "  - DB-25 + USB: 2293937\n", "  - RS-232: 2293938\n", "\n", "These specifications highlight the sensor's capabilities in terms of power handling, measurement accuracy, and operational requirements.\n"]}], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "Does my original retriever and chatbot code need any updates to support streaming responses?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Looking at your original code, you'll need a few modifications to properly support streaming responses. Here's what needs to be updated:\n", "\n", "## Streaming Support Modifications\n", "\n", "Your current implementation uses `chat_engine.chat()` which returns a complete response. For streaming, you'll need to use `chat_engine.stream_chat()` instead, which returns an object that can be asynchronously iterated over to get response chunks.\n", "\n", "Let's look at the specific changes needed:\n", "\n", "1. **Update the `ContextChatEngine` initialization**:\n", "\n", "The default `ContextChatEngine` configuration should work with streaming, but you might want to adjust a few parameters:\n", "\n", "```python\n", "def create_chat_engine(retriever):\n", "    memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n", "    return ContextChatEngine.from_defaults(\n", "        retriever=retriever,\n", "        memory=memory,\n", "        system_prompt=\"\"\"You are a helpful assistant specializing in laser measurement technology.\n", "        Answer questions based on the context provided. If you don't know the answer, say so.\"\"\",\n", "        # These parameters can help with streaming performance\n", "        streaming=True,\n", "        verbose=True\n", "    )\n", "```\n", "\n", "2. **Add a streaming method to test streaming functionality**:\n", "\n", "```python\n", "async def test_streaming():\n", "    chat_engine = init_chat_engine()\n", "    query = \"What are the specifications of the PM10K+ laser power sensor?\"\n", "    \n", "    # Get streaming response\n", "    streaming_response = chat_engine.stream_chat(query)\n", "    \n", "    # Print chunks as they arrive\n", "    print(\"Streaming response:\")\n", "    async for chunk in streaming_response.async_response_gen():\n", "        print(chunk, end=\"\", flush=True)\n", "    print(\"\\nStreaming complete!\")\n", "```\n", "\n", "3. **Make sure your LLM supports streaming**:\n", "\n", "Your current initialization of the OpenAI model looks good for streaming:\n", "\n", "```python\n", "llm = OpenAI(model=\"gpt-4o\", api_key=os.environ[\"OPENAI_API_KEY\"])\n", "```\n", "\n", "The OpenAI integration in LlamaIndex supports streaming by default, so this should work fine.\n", "\n", "## Additional Considerations\n", "\n", "1. **Error Handling for Streaming**:\n", "\n", "When implementing streaming in a web application, you should add proper error handling:\n", "\n", "```python\n", "async def stream_response(query):\n", "    try:\n", "        response = chat_engine.stream_chat(query)\n", "        async for chunk in response.async_response_gen():\n", "            yield chunk\n", "    except Exception as e:\n", "        yield f\"\\nError during streaming: {str(e)}\"\n", "```\n", "\n", "2. **Managing Context Window**:\n", "\n", "With streaming responses, you need to be careful about context window management. The `ChatMemoryBuffer` with a token limit helps, but you might want to add a function to clear chat history when needed:\n", "\n", "```python\n", "def reset_chat_memory(chat_engine):\n", "    chat_engine.memory.reset()\n", "    return \"Chat history has been cleared.\"\n", "```\n", "\n", "3. **Handling Large Retrieved Contexts**:\n", "\n", "If your retriever is bringing back a lot of context, it might slow down the initial response time before streaming starts. Consider adjusting the number of retrieved documents or implementing a more aggressive filtering mechanism:\n", "\n", "```python\n", "def create_retriever(cohere_api_key):\n", "    # Your existing code...\n", "    \n", "    return HybridRetrieverWithReranking(\n", "        vector_retriever=vector_retriever,\n", "        keyword_retriever=sqlite_retriever,\n", "        reranker=reranker,\n", "        vector_weight=0.7,\n", "        keyword_weight=0.3,\n", "        initial_top_k=10  # Reduced from 20 to improve streaming latency\n", "    )\n", "```\n", "\n", "Your core retrieval architecture (the hybrid approach with vector search, keyword search, and reranking) doesn't need fundamental changes to support streaming. The main updates are in how you initialize the chat engine and how you consume its responses.\n", "\n", "The streaming implementation looks good overall, and these adjustments should help you get the best streaming performance from your existing retriever and chatbot code."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "The development environment that I'm working in does not support async or streaming yet, so we will park the streaming for now. I will need to update the code to use streaming outputs later. For now we will proceed with the original code."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Understood! It makes sense to focus on getting your core functionality working first before adding streaming capabilities.\n", "\n", "For now, you can continue using your original implementation with `chat_engine.chat()` which works synchronously and returns complete responses. This approach is simpler to implement and debug in environments without async support.\n", "\n", "When you're ready to add streaming later, you can revisit the modifications we discussed:\n", "1. Updating to use `chat_engine.stream_chat()`\n", "2. Adding async handling for the response chunks\n", "3. Configuring the chat engine with streaming parameters\n", "\n", "Your current retriever architecture (hybrid approach with vector search, keyword search, and reranking) is solid and doesn't need changes regardless of whether you're using streaming or not. The core search and ranking functionality will work the same way.\n", "\n", "Let me know if you need any other adjustments to your current non-streaming implementation!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "We cannot run asyncio in this development environment. If we need async we can try wrapping it within the 'run_in_thread' function."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "I understand that you're working in an environment where directly running asyncio is causing issues. Let's modify our approach to use the `run_in_thread` function you've defined earlier for handling any async operations.\n", "\n", "Here's an updated version of the FastAPI application that should work in your environment:\n", "\n", "```python\n", "from fasthtml.common import *\n", "from fastapi import FastAPI, Depends, HTTPException\n", "from monsterui.all import *\n", "from pydantic import BaseModel\n", "from typing import List\n", "import os\n", "\n", "# Import your chat engine initialization function\n", "# chat_engine = init_chat_engine()\n", "\n", "# Initialize FastAPI app with FastHTML and MonsterUI\n", "app = FastAPI()\n", "html_app, rt = fast_app(hdrs=Theme.blue.headers())\n", "\n", "# Pydantic models for request/response\n", "class ChatMessage(BaseModel):\n", "    role: str\n", "    content: str\n", "\n", "class ChatRequest(BaseModel):\n", "    messages: List[ChatMessage]\n", "    query: str\n", "\n", "class ChatResponse(BaseModel):\n", "    response: str\n", "\n", "# In-memory message storage\n", "chat_history = []\n", "\n", "# FastAPI endpoint for chat\n", "@app.post(\"/api/chat\", response_model=ChatResponse)\n", "def chat(request: ChatRequest):\n", "    try:\n", "        # Process the query using chat engine\n", "        response = chat_engine.chat(request.query)\n", "        return {\"response\": response.response}\n", "    except Exception as e:\n", "        raise HTTPException(status_code=500, detail=str(e))\n", "\n", "# Mount FastHTML app to FastAPI\n", "app.mount(\"/\", html_app)\n", "\n", "# Simple test route to verify the server is running\n", "@rt(\"/\")\n", "def get():\n", "    return Titled(\"Laser Measurement Assistant\", \n", "                  P(\"API is running. Use /api/chat endpoint for queries.\"))\n", "```\n", "\n", "To run this server in your environment, we'll use the `run_in_thread` function to handle the async parts:\n", "\n", "```python\n", "# Function to start the server (to be used with run_in_thread)\n", "async def start_server():\n", "    import uvicorn\n", "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n", "    server = uvicorn.Server(config)\n", "    await server.serve()\n", "    \n", "# Start the server in a thread\n", "server_thread = threading.Thread(target=lambda: run_in_thread(start_server()))\n", "server_thread.daemon = True  # Make the thread terminate when the main program exits\n", "server_thread.start()\n", "\n", "print(\"Server started in background thread. Access at http://localhost:8000\")\n", "```\n", "\n", "For testing the endpoint, let's create a simple function that doesn't use async:\n", "\n", "```python\n", "def test_endpoint():\n", "    import requests\n", "    import json\n", "    import time\n", "    \n", "    # Give the server a moment to start\n", "    time.sleep(2)\n", "    \n", "    url = \"http://localhost:8000/api/chat\"\n", "    payload = {\n", "        \"messages\": [\n", "            {\"role\": \"user\", \"content\": \"Hello, I have a question about laser measurement.\"}\n", "        ],\n", "        \"query\": \"What are the specifications of the PM10K+ laser power sensor?\"\n", "    }\n", "    \n", "    headers = {\"Content-Type\": \"application/json\"}\n", "    try:\n", "        response = requests.post(url, data=json.dumps(payload), headers=headers)\n", "        \n", "        print(f\"Status Code: {response.status_code}\")\n", "        if response.status_code == 200:\n", "            print(\"Response:\")\n", "            print(json.dumps(response.json(), indent=2))\n", "        else:\n", "            print(f\"Error: {response.text}\")\n", "        return response\n", "    except Exception as e:\n", "        print(f\"Error: {str(e)}\")\n", "        return None\n", "\n", "# Run the test\n", "response = test_endpoint()\n", "```\n", "\n", "This approach should work in your environment by:\n", "\n", "1. Starting the FastAPI server in a separate thread using your `run_in_thread` function\n", "2. Testing the endpoint with a simple synchronous request\n", "\n", "If you still encounter issues, we can simplify further by:\n", "\n", "1. Creating just the API endpoint without FastHTML integration first\n", "2. Testing with a simple curl command or requests library\n", "3. Adding the UI components once the API is confirmed working\n", "\n", "Would you like me to provide a more simplified version focusing just on the API endpoint first?"]}, {"cell_type": "code", "metadata": {}, "source": ["from fasthtml.common import *\n", "from fastapi import FastAPI, Depends, HTTPException\n", "from monsterui.all import *\n", "from pydantic import BaseModel\n", "from typing import List\n", "import os\n", "\n", "# Import your chat engine initialization function\n", "# chat_engine = init_chat_engine()\n", "\n", "# Initialize FastAPI app with FastHTML and MonsterUI\n", "app = FastAPI()\n", "html_app, rt = fast_app(hdrs=Theme.blue.headers())\n", "\n", "# Pydantic models for request/response\n", "class ChatMessage(BaseModel):\n", "    role: str\n", "    content: str\n", "\n", "class ChatRequest(BaseModel):\n", "    messages: List[ChatMessage]\n", "    query: str\n", "\n", "class ChatResponse(BaseModel):\n", "    response: str\n", "\n", "# In-memory message storage\n", "chat_history = []\n", "\n", "# FastAPI endpoint for chat\n", "@app.post(\"/api/chat\", response_model=ChatResponse)\n", "def chat(request: ChatRequest):\n", "    try:\n", "        # Process the query using chat engine\n", "        response = chat_engine.chat(request.query)\n", "        return {\"response\": response.response}\n", "    except Exception as e:\n", "        raise HTTPException(status_code=500, detail=str(e))\n", "\n", "# Mount FastHTML app to FastAPI\n", "app.mount(\"/\", html_app)\n", "\n", "# Simple test route to verify the server is running\n", "@rt(\"/\")\n", "def get():\n", "    return Titled(\"Laser Measurement Assistant\", \n", "                  P(\"API is running. Use /api/chat endpoint for queries.\"))"], "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": ["# Function to start the server (to be used with run_in_thread)\n", "async def start_server():\n", "    import uvicorn\n", "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n", "    server = uvicorn.Server(config)\n", "    await server.serve()\n", "    \n", "# Start the server in a thread\n", "server_thread = threading.Thread(target=lambda: run_in_thread(start_server()))\n", "server_thread.daemon = True  # Make the thread terminate when the main program exits\n", "server_thread.start()\n", "\n", "print(\"Server started in background thread. Access at http://localhost:8000\")"], "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Server started in background thread. Access at http://localhost:8000\n"]}], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": ["def test_endpoint():\n", "    import requests\n", "    import json\n", "    import time\n", "    \n", "    # Give the server a moment to start\n", "    time.sleep(2)\n", "    \n", "    url = \"http://localhost:8000/api/chat\"\n", "    payload = {\n", "        \"messages\": [\n", "            {\"role\": \"user\", \"content\": \"Hello, I have a question about laser measurement.\"}\n", "        ],\n", "        \"query\": \"What are the specifications of the PM10K+ laser power sensor?\"\n", "    }\n", "    \n", "    headers = {\"Content-Type\": \"application/json\"}\n", "    try:\n", "        response = requests.post(url, data=json.dumps(payload), headers=headers)\n", "        \n", "        print(f\"Status Code: {response.status_code}\")\n", "        if response.status_code == 200:\n", "            print(\"Response:\")\n", "            print(json.dumps(response.json(), indent=2))\n", "        else:\n", "            print(f\"Error: {response.text}\")\n", "        return response\n", "    except Exception as e:\n", "        print(f\"Error: {str(e)}\")\n", "        return None\n", "\n", "# Run the test\n", "response = test_endpoint()"], "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Status Code: 200\n", "Response:\n", "{\n", "  \"response\": \"The specifications of the PM10K+ laser power sensor are as follows:\\n\\n- **Wavelength Range (\\u00b5m):** 0.19 to 11\\n- **Power Range (W):** 100 to 10,000\\n- **Maximum Intermittent Power (kW) (<5 min.):** 12 (beam size dependent)\\n- **Noise Equivalent Power (W) (at 6 lpm):**\\n  - USB and RS-232: <0.2\\n  - DB-25 with meter: <1.0\\n- **Maximum Power Density (kW/cm\\u00b2):**\\n  - 6.0 at 1 kW\\n  - 2.6 at 5 kW\\n  - 2.7 at 10 kW\\n  - 2.5 at 12 kW\\n- **Recommended Minimum Beam Size (mm):**\\n  - 6.5 at 1 kW\\n  - 22 at 5 kW\\n  - 31 at 10 kW\\n  - 35 at 12 kW\\n- **Minimum Water Flow Rate (lpm):** 6 (~10 PSI)\\n- **Water Temperature Range (\\u00b0C):** 10 to 25\\n- **Response Time (to 0 to 99%) (at 6 lpm):**\\n  - Speed-up On (seconds): <3\\n  - Speed-up Off (seconds): 10\\n- **Maximum Energy Density (mJ/cm\\u00b2) (1064 nm, 10 ns):** 600\\n- **Detector Coating:** BB+\\n- **Detector Dimensions (mm):** 65 x 65\\n- **Calibration Uncertainty (%):** \\u00b13\\n- **Power Linearity (%):** \\u00b12\\n- **Spectral Compensation Accuracy (%):** \\u00b11.5\\n- **Calibration Wavelength (nm):** 1070\\n- **Cooling Method:** Water\\n- **Interface Type:** DB-25 + USB, RS-232\\n- **Cable Length (m):** 2.0 (DB-25)\\n- **Part Numbers:**\\n  - DB-25 + USB: 2293937\\n  - RS-232: 2293938\\n\\nThese specifications highlight the sensor's capabilities in terms of power handling, measurement accuracy, and operational requirements.\"\n", "}\n"]}], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "Ok! Looks like that's working now! Anything else we need to doublecheck before moving on?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Great! The API endpoint is working correctly. Let's do a quick check of a few things before moving forward:\n", "\n", "1. **API Response Format**: The response looks good - it's properly formatted JSON with the expected structure.\n", "\n", "2. **Content Quality**: The response about the PM10K+ laser power sensor is detailed and well-formatted with bullet points, which means your retriever and chat engine are working correctly.\n", "\n", "3. **Server Running**: The server started successfully in the background thread, which confirms our threading approach works.\n", "\n", "Everything looks good from the API perspective! Here are a couple of things we might want to verify:\n", "\n", "1. **Error Handling**: You might want to test with an invalid query to ensure error handling works correctly.\n", "\n", "2. **Memory Usage**: If you plan to handle multiple users or long sessions, you might want to monitor memory usage during extended use.\n", "\n", "Now that the API endpoint is verified working, we can move on to building the FastHTML UI components. The next steps would be:\n", "\n", "1. Build the chat interface using MonsterUI components\n", "2. Connect the UI to the API endpoint\n", "3. Implement chat history management\n", "4. Add any additional features like message timestamps or user session management\n", "\n", "Would you like to start implementing the chat interface now?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "Yes, let's proceed with building the chat interface using FastHTML and MonsterUI components. I don't want it to have a dark background or a lot of color bubbles. I'd like it to look like ChatGPT or Perplexity or Anthropic user interfaces. Clean and sharp and light."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Let's build a clean, light-colored chat interface similar to ChatGPT, Perplexity, or Anthropic using FastHTML and MonsterUI. Here's the implementation:\n", "\n", "```python\n", "from fasthtml.common import *\n", "from monsterui.all import *\n", "\n", "# Update the route for the main page\n", "@rt(\"/\")\n", "def get():\n", "    return Titled(\"Laser Measurement Assistant\", \n", "                 chat_interface())\n", "\n", "def chat_interface():\n", "    \"\"\"Create a clean, light chat interface\"\"\"\n", "    return Div(\n", "        # Header\n", "        Div(\n", "            H1(\"Laser Measurement Assistant\", cls=\"text-xl font-medium\"),\n", "            P(\"Ask questions about laser measurement technology\", cls=TextPresets.muted_sm),\n", "            cls=\"border-b pb-4 mb-6\"\n", "        ),\n", "        \n", "        # Chat messages container\n", "        Div(\n", "            # Initial welcome message\n", "            message(\"Welcome to the Laser Measurement Assistant. How can I help you today?\", \"assistant\"),\n", "            id=\"chat-messages\",\n", "            cls=\"space-y-6 mb-6 overflow-y-auto max-h-[60vh]\"\n", "        ),\n", "        \n", "        # Input form\n", "        Form(\n", "            Div(\n", "                # Text input\n", "                TextArea(\n", "                    placeholder=\"Ask a question about laser measurement...\",\n", "                    id=\"user-input\",\n", "                    name=\"query\",\n", "                    rows=2,\n", "                    cls=\"w-full p-3 border rounded-md focus:outline-none focus:ring-2 focus:ring-blue-200 resize-none\"\n", "                ),\n", "                # Submit button\n", "                Button(\n", "                    \"Send\",\n", "                    type=\"submit\",\n", "                    cls=\"absolute right-3 bottom-3 px-4 py-2 bg-blue-500 text-white rounded-md hover:bg-blue-600 transition-colors\"\n", "                ),\n", "                cls=\"relative\"\n", "            ),\n", "            hx_post=\"/send-message\",\n", "            hx_target=\"#chat-messages\",\n", "            hx_swap=\"beforeend\",\n", "            hx_indicator=\"#spinner\",\n", "            cls=\"mt-4\"\n", "        ),\n", "        \n", "        # Loading indicator\n", "        Loading(cls=LoadingT.spinner, htmx_indicator=True, id=\"spinner\", \n", "                cls=\"fixed bottom-4 right-4 opacity-0\"),\n", "        \n", "        cls=\"max-w-3xl mx-auto p-6 bg-white rounded-lg shadow-sm\"\n", "    )\n", "\n", "def message(content, role):\n", "    \"\"\"Create a chat message component\"\"\"\n", "    is_user = role == \"user\"\n", "    \n", "    return Div(\n", "        # Avatar or icon\n", "        Div(\n", "            UkIcon(\"user\" if is_user else \"bot\", height=24, width=24),\n", "            cls=\"w-8 h-8 rounded-full bg-gray-100 flex items-center justify-center\"\n", "        ),\n", "        \n", "        # Message content\n", "        Div(\n", "            P(content, cls=\"whitespace-pre-wrap\"),\n", "            cls=\"flex-1\"\n", "        ),\n", "        \n", "        cls=f\"flex gap-4 items-start {'' if is_user else 'bg-gray-50 p-4 rounded-lg'}\"\n", "    )\n", "\n", "# Add route to handle message sending\n", "@rt(\"/send-message\", methods=[\"POST\"])\n", "def send_message(query: str):\n", "    \"\"\"Handle user message and get response\"\"\"\n", "    if not query.strip():\n", "        return \"\"\n", "    \n", "    # Create user message\n", "    user_message = message(query, \"user\")\n", "    \n", "    try:\n", "        # Get response from chat engine\n", "        response = chat_engine.chat(query)\n", "        \n", "        # Create assistant message\n", "        assistant_message = message(response.response, \"assistant\")\n", "        \n", "        # Clear input via OOB swap\n", "        clear_input = TextArea(\n", "            placeholder=\"Ask a question about laser measurement...\",\n", "            id=\"user-input\",\n", "            name=\"query\",\n", "            rows=2,\n", "            cls=\"w-full p-3 border rounded-md focus:outline-none focus:ring-2 focus:ring-blue-200 resize-none\",\n", "            hx_swap_oob=\"true\"\n", "        )\n", "        \n", "        return user_message, assistant_message, clear_input\n", "    \n", "    except Exception as e:\n", "        # Return error message\n", "        error_message = message(f\"Sorry, an error occurred: {str(e)}\", \"assistant\")\n", "        return user_message, error_message\n", "```\n", "\n", "Let's also add some CSS to make the interface even cleaner:\n", "\n", "```python\n", "# Add custom CSS to the headers\n", "custom_css = Style(\"\"\"\n", "    body {\n", "        background-color: #f9fafb;\n", "    }\n", "    \n", "    #chat-messages {\n", "        min-height: 300px;\n", "    }\n", "    \n", "    .message-assistant pre {\n", "        background-color: #f8f9fa;\n", "        padding: 1rem;\n", "        border-radius: 0.5rem;\n", "        overflow-x: auto;\n", "    }\n", "    \n", "    #user-input:focus {\n", "        box-shadow: none;\n", "        border-color: #3b82f6;\n", "    }\n", "    \n", "    /* Improve the typography */\n", "    p {\n", "        line-height: 1.6;\n", "    }\n", "    \n", "    /* Markdown styling */\n", "    .markdown h1, .markdown h2, .markdown h3 {\n", "        margin-top: 1.5rem;\n", "        margin-bottom: 1rem;\n", "    }\n", "    \n", "    .markdown ul, .markdown ol {\n", "        padding-left: 1.5rem;\n", "        margin-bottom: 1rem;\n", "    }\n", "    \n", "    .markdown ul li, .markdown ol li {\n", "        margin-bottom: 0.5rem;\n", "    }\n", "    \n", "    .markdown code {\n", "        background-color: #f1f5f9;\n", "        padding: 0.2rem 0.4rem;\n", "        border-radius: 0.25rem;\n", "        font-size: 0.875rem;\n", "    }\n", "\"\"\")\n", "\n", "# Update the FastHTML app to include the custom CSS\n", "html_app, rt = fast_app(hdrs=(Theme.blue.headers(), custom_css, MarkdownJS()))\n", "```\n", "\n", "This implementation features:\n", "\n", "1. **Clean, Light Design**: White background with minimal styling, similar to ChatGPT and other modern chat interfaces.\n", "\n", "2. **Message Styling**: \n", "   - User messages are simple with a small user icon\n", "   - Assistant messages have a subtle gray background to differentiate them\n", "   - Both have proper spacing and layout\n", "\n", "3. **Responsive Input Area**: \n", "   - A textarea that expands as needed\n", "   - Clear button styling with a blue accent color\n", "   - Focus states for better accessibility\n", "\n", "4. **Loading Indicator**: A spinner that appears when waiting for a response\n", "\n", "5. **Markdown Support**: Added MarkdownJS to render formatted text, which is important for displaying technical information\n", "\n", "6. **Proper Message Clearing**: The input field is cleared after sending a message\n", "\n", "Let me know if you'd like any adjustments to this design, such as:\n", "- Different color scheme\n", "- Alternative layout for messages\n", "- Additional features like a \"clear conversation\" button\n", "- Any other specific UI elements you'd prefer"]}, {"cell_type": "code", "metadata": {}, "source": ["from fasthtml.common import *\n", "from monsterui.all import *\n", "\n", "# Update the route for the main page\n", "@rt(\"/\")\n", "def get():\n", "    return Titled(\"Laser Measurement Assistant\", \n", "                 chat_interface())\n", "\n", "def chat_interface():\n", "    \"\"\"Create a clean, light chat interface\"\"\"\n", "    return Div(\n", "        # Header\n", "        Div(\n", "            H1(\"Laser Measurement Assistant\", cls=\"text-xl font-medium\"),\n", "            P(\"Ask questions about laser measurement technology\", cls=TextPresets.muted_sm),\n", "            cls=\"border-b pb-4 mb-6\"\n", "        ),\n", "        \n", "        # Chat messages container\n", "        Div(\n", "            # Initial welcome message\n", "            message(\"Welcome to the Laser Measurement Assistant. How can I help you today?\", \"assistant\"),\n", "            id=\"chat-messages\",\n", "            cls=\"space-y-6 mb-6 overflow-y-auto max-h-[60vh]\"\n", "        ),\n", "        \n", "        # Input form\n", "        Form(\n", "            Div(\n", "                # Text input\n", "                TextArea(\n", "                    placeholder=\"Ask a question about laser measurement...\",\n", "                    id=\"user-input\",\n", "                    name=\"query\",\n", "                    rows=2,\n", "                    cls=\"w-full p-3 border rounded-md focus:outline-none focus:ring-2 focus:ring-blue-200 resize-none\"\n", "                ),\n", "                # Submit button\n", "                Button(\n", "                    \"Send\",\n", "                    type=\"submit\",\n", "                    cls=\"absolute right-3 bottom-3 px-4 py-2 bg-blue-500 text-white rounded-md hover:bg-blue-600 transition-colors\"\n", "                ),\n", "                cls=\"relative\"\n", "            ),\n", "            hx_post=\"/send-message\",\n", "            hx_target=\"#chat-messages\",\n", "            hx_swap=\"beforeend\",\n", "            hx_indicator=\"#spinner\",\n", "            cls=\"mt-4\"\n", "        ),\n", "        \n", "        # Loading indicator - Fixed the duplicate cls parameter\n", "        Loading(\n", "            cls=(LoadingT.spinner, \"fixed bottom-4 right-4 opacity-0\"), \n", "            htmx_indicator=True, \n", "            id=\"spinner\"\n", "        ),\n", "        \n", "        cls=\"max-w-3xl mx-auto p-6 bg-white rounded-lg shadow-sm\"\n", "    )\n", "\n", "def message(content, role):\n", "    \"\"\"Create a chat message component\"\"\"\n", "    is_user = role == \"user\"\n", "    \n", "    return Div(\n", "        # Avatar or icon\n", "        Div(\n", "            UkIcon(\"user\" if is_user else \"bot\", height=24, width=24),\n", "            cls=\"w-8 h-8 rounded-full bg-gray-100 flex items-center justify-center\"\n", "        ),\n", "        \n", "        # Message content\n", "        Div(\n", "            P(content, cls=\"whitespace-pre-wrap\"),\n", "            cls=\"flex-1\"\n", "        ),\n", "        \n", "        cls=f\"flex gap-4 items-start {'' if is_user else 'bg-gray-50 p-4 rounded-lg'}\"\n", "    )\n", "\n", "# Add route to handle message sending\n", "@rt(\"/send-message\", methods=[\"POST\"])\n", "def send_message(query: str):\n", "    \"\"\"Handle user message and get response\"\"\"\n", "    if not query.strip():\n", "        return \"\"\n", "    \n", "    # Create user message\n", "    user_message = message(query, \"user\")\n", "    \n", "    try:\n", "        # Get response from chat engine\n", "        response = chat_engine.chat(query)\n", "        \n", "        # Create assistant message\n", "        assistant_message = message(response.response, \"assistant\")\n", "        \n", "        # Clear input via OOB swap\n", "        clear_input = TextArea(\n", "            placeholder=\"Ask a question about laser measurement...\",\n", "            id=\"user-input\",\n", "            name=\"query\",\n", "            rows=2,\n", "            cls=\"w-full p-3 border rounded-md focus:outline-none focus:ring-2 focus:ring-blue-200 resize-none\",\n", "            hx_swap_oob=\"true\"\n", "        )\n", "        \n", "        return user_message, assistant_message, clear_input\n", "    \n", "    except Exception as e:\n", "        # Return error message\n", "        error_message = message(f\"Sorry, an error occurred: {str(e)}\", \"assistant\")\n", "        return user_message, error_message"], "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "Let's test it before doing custom css styling."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Good idea! Let's test the interface as it is before adding custom CSS styling. This will help us see how the basic components work together and identify any issues that need to be fixed.\n", "\n", "To test the interface:\n", "\n", "1. Make sure the FastAPI app with the new routes is running (using the same approach we used earlier with the `run_in_thread` function)\n", "\n", "2. Visit the main page at http://localhost:8000/ to see the chat interface\n", "\n", "3. Try sending a test message to see if the chat functionality works correctly\n", "\n", "Let's update our server code to include the new routes and restart it:\n", "\n", "```python\n", "# Restart the server with the updated routes\n", "async def restart_server():\n", "    # First, let's initialize the chat engine if not already done\n", "    global chat_engine\n", "    if 'chat_engine' not in globals():\n", "        chat_engine = init_chat_engine()\n", "    \n", "    import uvicorn\n", "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n", "    server = uvicorn.Server(config)\n", "    await server.serve()\n", "\n", "# Stop previous server thread if it exists\n", "if 'server_thread' in globals():\n", "    # No direct way to stop the thread, but we can create a new one\n", "    print(\"Restarting server...\")\n", "\n", "# Start a new server thread\n", "server_thread = threading.Thread(target=lambda: run_in_thread(restart_server()))\n", "server_thread.daemon = True\n", "server_thread.start()\n", "\n", "print(\"Server restarted in background thread. Access at http://localhost:8000\")\n", "```\n", "\n", "After testing the basic interface, we can then decide what CSS customizations would be most beneficial. This approach allows us to:\n", "\n", "1. Verify that the core functionality works\n", "2. See what the interface looks like with just the default styling\n", "3. Make more informed decisions about what CSS changes are needed\n", "\n", "Let me know what you observe when testing the interface, and we can make adjustments as needed!"]}, {"cell_type": "code", "metadata": {}, "source": ["# Restart the server with the updated routes\n", "async def restart_server():\n", "    # First, let's initialize the chat engine if not already done\n", "    global chat_engine\n", "    if 'chat_engine' not in globals():\n", "        chat_engine = init_chat_engine()\n", "    \n", "    import uvicorn\n", "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n", "    server = uvicorn.Server(config)\n", "    await server.serve()\n", "\n", "# Stop previous server thread if it exists\n", "if 'server_thread' in globals():\n", "    # No direct way to stop the thread, but we can create a new one\n", "    print(\"Restarting server...\")\n", "\n", "# Start a new server thread\n", "server_thread = threading.Thread(target=lambda: run_in_thread(restart_server()))\n", "server_thread.daemon = True\n", "server_thread.start()\n", "\n", "print(\"Server restarted in background thread. Access at http://localhost:8000\")"], "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Restarting server...\n", "Server restarted in background thread. Access at http://localhost:8000\n"]}], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "The only thing about accessing the local host is that this is a cloud development server environment. I need to access it from my home PC to test it."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "I understand - you're running this in a cloud development environment and need to access it from your home PC. Let's adjust our approach for this scenario.\n", "\n", "## Options for Accessing from Your Home PC\n", "\n", "1. **Port Forwarding / Tunneling**:\n", "   You can use a service like ngrok, localtunnel, or cloudflared to create a temporary public URL that forwards to your local server.\n", "\n", "2. **Public IP Configuration**:\n", "   If your cloud environment has a public IP, we need to:\n", "   - Make sure the server binds to `0.0.0.0` (all interfaces) instead of just localhost\n", "   - Configure any necessary firewall rules to allow incoming connections\n", "\n", "Let's set up a tunnel using a common service:\n", "\n", "```python\n", "# Install a tunneling service if needed\n", "!pip install pyngrok\n", "\n", "# Import and set up ngrok\n", "from pyngrok import ngrok\n", "\n", "# Restart the server binding to all interfaces\n", "async def restart_server_public():\n", "    # First, let's initialize the chat engine if not already done\n", "    global chat_engine\n", "    if 'chat_engine' not in globals():\n", "        chat_engine = init_chat_engine()\n", "    \n", "    import uvicorn\n", "    # Bind to 0.0.0.0 to make it accessible externally\n", "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n", "    server = uvicorn.Server(config)\n", "    await server.serve()\n", "\n", "# Start a new server thread\n", "server_thread = threading.Thread(target=lambda: run_in_thread(restart_server_public()))\n", "server_thread.daemon = True\n", "server_thread.start()\n", "\n", "# Set up an ngrok tunnel to the server\n", "# This creates a public URL that forwards to your local server\n", "public_url = ngrok.connect(8000).public_url\n", "print(f\"Server accessible at: {public_url}\")\n", "```\n", "\n", "Alternatively, if your cloud environment already provides a public URL or IP:\n", "\n", "```python\n", "# Get the public IP of the cloud environment\n", "import requests\n", "\n", "try:\n", "    # This gets the public IP of the machine\n", "    public_ip = requests.get('https://api.ipify.org').text\n", "    print(f\"Server should be accessible at: http://{public_ip}:8000\")\n", "    print(\"Note: Make sure any firewall rules allow incoming connections on port 8000\")\n", "except:\n", "    print(\"Could not determine public IP. Check your cloud environment's dashboard for connection details.\")\n", "```\n", "\n", "## Testing Without Direct Access\n", "\n", "If setting up public access is challenging, we can also create a simple test function that simulates user interaction with the chat interface:\n", "\n", "```python\n", "def test_chat_interface():\n", "    \"\"\"Test the chat interface programmatically\"\"\"\n", "    import requests\n", "    \n", "    # Base URL of the server\n", "    base_url = \"http://localhost:8000\"\n", "    \n", "    # Test questions\n", "    test_questions = [\n", "        \"What are the specifications of the PM10K+ laser power sensor?\",\n", "        \"How does the water cooling system work?\",\n", "        \"What is the maximum power the sensor can handle?\"\n", "    ]\n", "    \n", "    print(\"Testing chat interface with sample questions:\")\n", "    for i, question in enumerate(test_questions):\n", "        print(f\"\\nQuestion {i+1}: {question}\")\n", "        \n", "        # Send the question to the /send-message endpoint\n", "        response = requests.post(\n", "            f\"{base_url}/send-message\",\n", "            data={\"query\": question}\n", "        )\n", "        \n", "        # Check if request was successful\n", "        if response.status_code == 200:\n", "            print(f\"Response received (length: {len(response.text)} characters)\")\n", "            # Print a preview of the response\n", "            preview = response.text[:100] + \"...\" if len(response.text) > 100 else response.text\n", "            print(f\"Preview: {preview}\")\n", "        else:\n", "            print(f\"Error: {response.status_code}\")\n", "            print(response.text)\n", "    \n", "    print(\"\\nTesting complete!\")\n", "\n", "# Run the test\n", "test_chat_interface()\n", "```\n", "\n", "Let me know which approach works best for your environment, and we can proceed accordingly!"]}, {"cell_type": "code", "metadata": {}, "source": ["# Install a tunneling service if needed\n", "!pip install pyngrok\n", "\n", "# Import and set up ngrok\n", "from pyngrok import ngrok\n", "\n", "# Restart the server binding to all interfaces\n", "async def restart_server_public():\n", "    # First, let's initialize the chat engine if not already done\n", "    global chat_engine\n", "    if 'chat_engine' not in globals():\n", "        chat_engine = init_chat_engine()\n", "    \n", "    import uvicorn\n", "    # Bind to 0.0.0.0 to make it accessible externally\n", "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n", "    server = uvicorn.Server(config)\n", "    await server.serve()\n", "\n", "# Start a new server thread\n", "server_thread = threading.Thread(target=lambda: run_in_thread(restart_server_public()))\n", "server_thread.daemon = True\n", "server_thread.start()\n", "\n", "# Set up an ngrok tunnel to the server\n", "# This creates a public URL that forwards to your local server\n", "public_url = ngrok.connect(8000).public_url\n", "print(f\"Server accessible at: {public_url}\")"], "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Defaulting to user installation because normal site-packages is not writeable\r\n", "Collecting pyngrok\r\n", "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\r\n", "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/site-packages (from pyngrok) (6.0.2)\r\n", "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\r\n", "Installing collected packages: pyngrok\r\n", "Successfully installed pyngrok-7.2.3\r\n", "\r\n", "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\r\n", "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n", "Downloading ngrok ...\r", "Downloading ngrok: 0%\r", "Downloading ngrok: 1%\r", "Downloading ngrok: 2%\r", "Downloading ngrok: 3%\r", "Downloading ngrok: 4%\r", "Downloading ngrok: 5%\r", "Downloading ngrok: 6%\r", "Downloading ngrok: 7%\r", "Downloading ngrok: 8%\r", "Downloading ngrok: 9%\r", "Downloading ngrok: 10%\r", "Downloading ngrok: 11%\r", "Downloading ngrok: 12%\r", "Downloading ngrok: 13%\r", "Downloading ngrok: 14%\r", "Downloading ngrok: 15%\r", "Downloading ngrok: 16%\r", "Downloading ngrok: 17%\r", "Downloading ngrok: 18%\r", "Downloading ngrok: 19%\r", "Downloading ngrok: 20%\r", "Downloading ngrok: 21%\r", "Downloading ngrok: 22%\r", "Downloading ngrok: 23%\r", "Downloading ngrok: 24%\r", "Downloading ngrok: 25%\r", "Downloading ngrok: 26%\r", "Downloading ngrok: 27%\r", "Downloading ngrok: 28%\r", "Downloading ngrok: 29%\r", "Downloading ngrok: 30%\r", "Downloading ngrok: 31%\r", "Downloading ngrok: 32%\r", "Downloading ngrok: 33%\r", "Downloading ngrok: 34%\r", "Downloading ngrok: 35%\r", "Downloading ngrok: 36%\r", "Downloading ngrok: 37%\r", "Downloading ngrok: 38%\r", "Downloading ngrok: 39%\r", "Downloading ngrok: 40%\r", "Downloading ngrok: 41%\r", "Downloading ngrok: 42%\r", "Downloading ngrok: 43%\r", "Downloading ngrok: 44%\r", "Downloading ngrok: 45%\r", "Downloading ngrok: 46%\r", "Downloading ngrok: 47%\r", "Downloading ngrok: 48%\r", "Downloading ngrok: 49%\r", "Downloading ngrok: 50%\r", "Downloading ngrok: 51%\r", "Downloading ngrok: 52%\r", "Downloading ngrok: 53%\r", "Downloading ngrok: 54%\r", "Downloading ngrok: 55%\r", "Downloading ngrok: 56%\r", "Downloading ngrok: 57%\r", "Downloading ngrok: 58%\r", "Downloading ngrok: 59%\r", "Downloading ngrok: 60%\r", "Downloading ngrok: 61%\r", "Downloading ngrok: 62%\r", "Downloading ngrok: 63%\r", "Downloading ngrok: 64%\r", "Downloading ngrok: 65%\r", "Downloading ngrok: 66%\r", "Downloading ngrok: 67%\r", "Downloading ngrok: 68%\r", "Downloading ngrok: 69%\r", "Downloading ngrok: 70%\r", "Downloading ngrok: 71%\r", "Downloading ngrok: 72%\r", "Downloading ngrok: 73%\r", "Downloading ngrok: 74%\r", "Downloading ngrok: 75%\r", "Downloading ngrok: 76%\r", "Downloading ngrok: 77%\r", "Downloading ngrok: 78%\r", "Downloading ngrok: 79%\r", "Downloading ngrok: 80%\r", "Downloading ngrok: 81%\r", "Downloading ngrok: 82%\r", "Downloading ngrok: 83%\r", "Downloading ngrok: 84%\r", "Downloading ngrok: 85%\r", "Downloading ngrok: 86%\r", "Downloading ngrok: 87%\r", "Downloading ngrok: 88%\r", "Downloading ngrok: 89%\r", "Downloading ngrok: 90%\r", "Downloading ngrok: 91%\r", "Downloading ngrok: 92%\r", "Downloading ngrok: 93%\r", "Downloading ngrok: 94%\r", "Downloading ngrok: 95%\r", "Downloading ngrok: 96%\r", "Downloading ngrok: 97%\r", "Downloading ngrok: 98%\r", "Downloading ngrok: 99%\r", "Downloading ngrok: 100%\r", "                                                                                                    \r", "Installing ngrok ... \r", "                                                                                                    \r", "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n", "\u001b[31mPyngrokNgrokError\u001b[39m                         Traceback (most recent call last)\n", "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 27\u001b[39m\n", "\u001b[32m     23\u001b[39m server_thread.start()\n", "\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Set up an ngrok tunnel to the server\u001b[39;00m\n", "\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# This creates a public URL that forwards to your local server\u001b[39;00m\n", "\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m public_url = \u001b[43mngrok\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m8000\u001b[39;49m\u001b[43m)\u001b[49m.public_url\n", "\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mServer accessible at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpublic_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n", "\n", "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pyngrok/ngrok.py:351\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(addr, proto, name, pyngrok_config, **options)\u001b[39m\n", "\u001b[32m    347\u001b[39m _upgrade_legacy_params(pyngrok_config, options)\n", "\u001b[32m    349\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOpening tunnel named: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n", "\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m api_url = \u001b[43mget_ngrok_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m.api_url\n", "\u001b[32m    353\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreating tunnel with options: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n", "\u001b[32m    355\u001b[39m tunnel = NgrokTunnel(api_request(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/api/tunnels\u001b[39m\u001b[33m\"\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m, data=options,\n", "\u001b[32m    356\u001b[39m                                  timeout=pyngrok_config.request_timeout),\n", "\u001b[32m    357\u001b[39m                      pyngrok_config, api_url)\n", "\n", "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pyngrok/ngrok.py:176\u001b[39m, in \u001b[36mget_ngrok_process\u001b[39m\u001b[34m(pyngrok_config)\u001b[39m\n", "\u001b[32m    172\u001b[39m     pyngrok_config = conf.get_default()\n", "\u001b[32m    174\u001b[39m install_ngrok(pyngrok_config)\n", "\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m\n", "\n", "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pyngrok/process.py:265\u001b[39m, in \u001b[36mget_process\u001b[39m\u001b[34m(pyngrok_config)\u001b[39m\n", "\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_process_running(pyngrok_config.ngrok_path):\n", "\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _current_processes[pyngrok_config.ngrok_path]\n", "\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_start_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m\n", "\n", "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pyngrok/process.py:428\u001b[39m, in \u001b[36m_start_process\u001b[39m\u001b[34m(pyngrok_config)\u001b[39m\n", "\u001b[32m    425\u001b[39m kill_process(pyngrok_config.ngrok_path)\n", "\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ngrok_process.startup_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n", "\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PyngrokNgrokError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe ngrok process errored on start: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mngrok_process.startup_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n", "\u001b[32m    429\u001b[39m                             ngrok_process.logs,\n", "\u001b[32m    430\u001b[39m                             ngrok_process.startup_error)\n", "\u001b[32m    431\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n", "\u001b[32m    432\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PyngrokNgrokError(\u001b[33m\"\u001b[39m\u001b[33mThe ngrok process was unable to start.\u001b[39m\u001b[33m\"\u001b[39m, ngrok_process.logs)\n", "\n", "\u001b[31mPyngrokNgrokError\u001b[39m: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.\n"]}, {"name": "stderr", "output_type": "stream", "text": ["/usr/local/lib/python3.11/site-packages/lancedb/__init__.py:220: UserWarning: lance is not fork-safe. If you are using multiprocessing, use spawn instead.\n", "  warnings.warn(\n", "INFO:     Started server process [8]\n", "INFO:     Waiting for application startup.\n", "INFO:     Application startup complete.\n", "ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 8000): address already in use\n", "INFO:     Waiting for application shutdown.\n", "INFO:     Application shutdown complete.\n", "t=2025-03-31T00:53:20+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n", "t=2025-03-31T00:53:20+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n", "t=2025-03-31T00:53:20+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n", "t=2025-03-31T00:53:20+0000 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context canceled\"\n", "t=2025-03-31T00:53:20+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"]}, {"ename": "PyngrokNgrokError", "evalue": "The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.", "output_type": "error", "traceback": ["Traceback (most recent call last):\n", "  File \"/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3549, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n", "  File \"<ipython-input-1-c56a6ed987b9>\", line 27, in <module>\n    public_url = ngrok.connect(8000).public_url\n                 ^^^^^^^^^^^^^^^^^^^\n", "  File \"/app/data/.local/lib/python3.11/site-packages/pyngrok/ngrok.py\", line 351, in connect\n    api_url = get_ngrok_process(pyngrok_config).api_url\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n", "  File \"/app/data/.local/lib/python3.11/site-packages/pyngrok/ngrok.py\", line 176, in get_ngrok_process\n    return process.get_process(pyngrok_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n", "  File \"/app/data/.local/lib/python3.11/site-packages/pyngrok/process.py\", line 265, in get_process\n    return _start_process(pyngrok_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n", "  File \"/app/data/.local/lib/python3.11/site-packages/pyngrok/process.py\", line 428, in _start_process\n    raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n", "pyngrok.exception.PyngrokNgrokError: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.\n"]}], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Prompt**\n", "\n", "I need to setup a ngrok account for that to work. But I just remembered I signed up for the Plash service which answer.ai has released to deploy FastHTML apps. I have inserted information about this plash service into your context. Let's see if you can deploy this app onto Plash. I have inserted the PLASH_TOKEN and PLASH_EMAIL as environment variables."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AI Response**\n", "\n", "Great! Let's deploy your FastHTML application to Plash. Based on the documentation provided, Plash is designed specifically for FastHTML apps, which makes it a perfect fit for our project.\n", "\n", "Here's how we'll deploy your application to Plash:\n", "\n", "## Step 1: Organize Project Files\n", "\n", "First, let's create the necessary project structure:\n", "\n", "```python\n", "# Create a directory structure for Plash deployment\n", "!mkdir -p laser_assistant\n", "```\n", "\n", "Now, let's create the main application file:\n", "\n", "```python\n", "# Create the main.py file for our app\n", "with open('laser_assistant/main.py', 'w') as f:\n", "    f.write('''\n", "from fasthtml.common import *\n", "from monsterui.all import *\n", "import os\n", "import pickle\n", "import sqlite3\n", "from typing import List\n", "from llama_index.core.schema import NodeWithScore, TextNode, QueryBundle\n", "from llama_index.core import StorageContext, VectorStoreIndex\n", "from llama_index.core.retrievers import BaseRetriever\n", "from llama_index.core.memory import ChatMemoryBuffer\n", "from llama_index.core.chat_engine import ContextChatEngine\n", "from qdrant_client import QdrantClient\n", "from llama_index.vector_stores.qdrant import QdrantVectorStore\n", "from llama_index.postprocessor.cohere_rerank import CohereRerank\n", "from llama_index.llms.openai import OpenAI\n", "from llama_index.embeddings.openai import OpenAIEmbedding\n", "from llama_index.core import Settings\n", "\n", "# Initialize LLM and embeddings\n", "llm = OpenAI(model=\"gpt-4o\", api_key=os.environ[\"OPENAI_API_KEY\"])\n", "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n", "Settings.llm = llm\n", "Settings.embed_model = embed_model\n", "\n", "# SQLite FTS Retriever\n", "class SQLiteFTSRetriever:\n", "    def __init__(self, db_path=\"laser_nodes.db\", top_k=5):\n", "        self.db_path = db_path\n", "        self.top_k = top_k\n", "        \n", "    def retrieve(self, query_str: str) -> List[NodeWithScore]:\n", "        conn = sqlite3.connect(self.db_path)\n", "        c = conn.cursor()\n", "        escaped_query = f'\"{query_str}\"'\n", "        c.execute(\n", "            f\"\"\"\n", "            SELECT nodes.node_id, nodes.content, nodes.metadata, nodes_fts.rank\n", "            FROM nodes_fts \n", "            JOIN nodes ON nodes_fts.rowid = nodes.rowid\n", "            WHERE nodes_fts MATCH ? \n", "            ORDER BY nodes_fts.rank\n", "            LIMIT {self.top_k}\n", "            \"\"\", \n", "            (escaped_query,)\n", "        )\n", "        \n", "        results = []\n", "        for node_id, content, metadata_blob, rank in c.fetchall():\n", "            import json\n", "            metadata = json.loads(metadata_blob)\n", "            node = TextNode(text=content, metadata=metadata, id_=node_id)\n", "            score = 1.0 / (1.0 + float(rank))\n", "            results.append(NodeWithScore(node=node, score=score))\n", "        \n", "        conn.close()\n", "        return results\n", "\n", "# Hybrid Retriever with Reranking\n", "class HybridRetrieverWithReranking(BaseRetriever):\n", "    def __init__(self, vector_retriever, keyword_retriever, reranker, \n", "                vector_weight=0.7, keyword_weight=0.3, initial_top_k=20):\n", "        self.vector_retriever = vector_retriever\n", "        self.keyword_retriever = keyword_retriever\n", "        self.reranker = reranker\n", "        self.vector_weight = vector_weight\n", "        self.keyword_weight = keyword_weight\n", "        self.initial_top_k = initial_top_k\n", "        super().__init__()\n", "    \n", "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n", "        query_str = query_bundle.query_str\n", "        \n", "        # Get results from both retrievers\n", "        vector_results = self.vector_retriever.retrieve(query_str)\n", "        keyword_results = self.keyword_retriever.retrieve(query_str)\n", "        \n", "        # Combine scores\n", "        node_scores = {}\n", "        for i, result in enumerate(vector_results):\n", "            node_id = result.node.node_id\n", "            score = self.vector_weight * (1.0 / (i + 1))\n", "            node_scores[node_id] = {\"node\": result.node, \"score\": score}\n", "        \n", "        for i, result in enumerate(keyword_results):\n", "            node_id = result.node.node_id\n", "            keyword_score = self.keyword_weight * (1.0 / (i + 1))\n", "            if node_id in node_scores:\n", "                node_scores[node_id][\"score\"] += keyword_score\n", "            else:\n", "                node_scores[node_id] = {\"node\": result.node, \"score\": keyword_score}\n", "        \n", "        # Sort by score\n", "        sorted_results = sorted(\n", "            node_scores.values(), \n", "            key=lambda x: x[\"score\"], \n", "            reverse=True\n", "        )\n", "        \n", "        # Convert to NodeWithScore objects\n", "        initial_results = [\n", "            NodeWithScore(node=item[\"node\"], score=item[\"score\"]) \n", "            for item in sorted_results[:self.initial_top_k]\n", "        ]\n", "        \n", "        # Apply reranking\n", "        reranked_nodes = self.reranker.postprocess_nodes(\n", "            initial_results, \n", "            query_bundle\n", "        )\n", "        \n", "        return reranked_nodes\n", "\n", "# Load function for vector stores and create retriever\n", "def create_retriever(cohere_api_key):\n", "    # Load SQLite retriever\n", "    sqlite_retriever = SQLiteFTSRetriever()\n", "    \n", "    # Load Qdrant collection\n", "    with open('laser_qdrant.pkl', 'rb') as f:\n", "        collection_data = pickle.load(f)\n", "    \n", "    # Create Qdrant client and collection\n", "    qdrant_client = QdrantClient(\":memory:\")\n", "    collection_name = collection_data[\"collection_name\"]\n", "    \n", "    if qdrant_client.collection_exists(collection_name):\n", "        qdrant_client.delete_collection(collection_name)\n", "    \n", "    qdrant_client.create_collection(\n", "        collection_name=collection_name,\n", "        vectors_config={\"size\": collection_data[\"vector_size\"], \"distance\": \"Cosine\"}\n", "    )\n", "    \n", "    if collection_data[\"points\"]:\n", "        qdrant_client.upsert(\n", "            collection_name=collection_name,\n", "            points=collection_data[\"points\"]\n", "        )\n", "    \n", "    # Create vector store and retriever\n", "    vector_store = QdrantVectorStore(\n", "        client=qdrant_client,\n", "        collection_name=collection_name\n", "    )\n", "    \n", "    index = VectorStoreIndex.from_vector_store(vector_store)\n", "    vector_retriever = index.as_retriever(similarity_top_k=10)\n", "    \n", "    # Create Cohere reranker\n", "    reranker = CohereRerank(\n", "        api_key=cohere_api_key,\n", "        model=\"rerank-v3.5\",\n", "        top_n=5\n", "    )\n", "    \n", "    # Create and return hybrid retriever with reranking\n", "    return HybridRetrieverWithReranking(\n", "        vector_retriever=vector_retriever,\n", "        keyword_retriever=sqlite_retriever,\n", "        reranker=reranker,\n", "        vector_weight=0.7,\n", "        keyword_weight=0.3,\n", "        initial_top_k=20\n", "    )\n", "\n", "# Create chat engine\n", "def create_chat_engine(retriever):\n", "    memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n", "    return ContextChatEngine.from_defaults(\n", "        retriever=retriever,\n", "        memory=memory,\n", "        system_prompt=\"\"\"You are a helpful assistant specializing in laser measurement technology.\n", "        Answer questions based on the context provided. If you don't know the answer, say so.\"\"\"\n", "    )\n", "\n", "# Main initialization function\n", "def init_chat_engine():\n", "    cohere_api_key = os.environ.get(\"COHERE_API_KEY\")\n", "    retriever = create_retriever(cohere_api_key)\n", "    return create_chat_engine(retriever)\n", "\n", "# Initialize the chat engine\n", "chat_engine = init_chat_engine()\n", "\n", "# Create the FastHTML app\n", "app, rt = fast_app(hdrs=Theme.blue.headers())\n", "\n", "# Define message component\n", "def message(content, role):\n", "    \"\"\"Create a chat message component\"\"\"\n", "    is_user = role == \"user\"\n", "    \n", "    return Div(\n", "        # Avatar or icon\n", "        Div(\n", "            UkIcon(\"user\" if is_user else \"bot\", height=24, width=24),\n", "            cls=\"w-8 h-8 rounded-full bg-gray-100 flex items-center justify-center\"\n", "        ),\n", "        \n", "        # Message content\n", "        Div(\n", "            P(content, cls=\"whitespace-pre-wrap\"),\n", "            cls=\"flex-1\"\n", "        ),\n", "        \n", "        cls=f\"flex gap-4 items-start {'' if is_user else 'bg-gray-50 p-4 rounded-lg'}\"\n", "    )\n", "\n", "# Define chat interface\n", "def chat_interface():\n", "    \"\"\"Create a clean, light chat interface\"\"\"\n", "    return Div(\n", "        # Header\n", "        Div(\n", "            H1(\"Laser Measurement Assistant\", cls=\"text-xl font-medium\"),\n", "            P(\"Ask questions about laser measurement technology\", cls=TextPresets.muted_sm),\n", "            cls=\"border-b pb-4 mb-6\"\n", "        ),\n", "        \n", "        # Chat messages container\n", "        Div(\n", "            # Initial welcome message\n", "            message(\"Welcome to the Laser Measurement Assistant. How can I help you today?\", \"assistant\"),\n", "            id=\"chat-messages\",\n", "            cls=\"space-y-6 mb-6 overflow-y-auto max-h-[60vh]\"\n", "        ),\n", "        \n", "        # Input form\n", "        Form(\n", "            Div(\n", "                # Text input\n", "                TextArea(\n", "                    placeholder=\"Ask a question about laser measurement...\",\n", "                    id=\"user-input\",\n", "                    name=\"query\",\n", "                    rows=2,\n", "                    cls=\"w-full p-3 border rounded-md focus:outline-none focus:ring-2 focus:ring-blue-200 resize-none\"\n", "                ),\n", "                # Submit button\n", "                Button(\n", "                    \"Send\",\n", "                    type=\"submit\",\n", "                    cls=\"absolute right-3 bottom-3 px-4 py-2 bg-blue-500 text-white rounded-md hover:bg-blue-600 transition-colors\"\n", "                ),\n", "                cls=\"relative\"\n", "            ),\n", "            hx_post=\"/send-message\",\n", "            hx_target=\"#chat-messages\",\n", "            hx_swap=\"beforeend\",\n", "            hx_indicator=\"#spinner\",\n", "            cls=\"mt-4\"\n", "        ),\n", "        \n", "        # Loading indicator\n", "        Loading(\n", "            cls=(LoadingT.spinner, \"fixed bottom-4 right-4 opacity-0\"), \n", "            htmx_indicator=True, \n", "            id=\"spinner\"\n", "        ),\n", "        \n", "        cls=\"max-w-3xl mx-auto p-6 bg-white rounded-lg shadow-sm\"\n", "    )\n", "\n", "# Define routes\n", "@rt(\"/\")\n", "def get():\n", "    return Titled(\"Laser Measurement Assistant\", chat_interface())\n", "\n", "@rt(\"/send-message\", methods=[\"POST\"])\n", "def send_message(query: str):\n", "    \"\"\"Handle user message and get response\"\"\"\n", "    if not query.strip():\n", "        return \"\"\n", "    \n", "    # Create user message\n", "    user_message = message(query, \"user\")\n", "    \n", "    try:\n", "        # Get response from chat engine\n", "        response = chat_engine.chat(query)\n", "        \n", "        # Create assistant message\n", "        assistant_message = message(response.response, \"assistant\")\n", "        \n", "        # Clear input via OOB swap\n", "        clear_input = TextArea(\n", "            placeholder=\"Ask a question about laser measurement...\",\n", "            id=\"user-input\",\n", "            name=\"query\",\n", "            rows=2,\n", "            cls=\"w-full p-3 border rounded-md focus:outline-none focus:ring-2 focus:ring-blue-200 resize-none\",\n", "            hx_swap_oob=\"true\"\n", "        )\n", "        \n", "        return user_message, assistant_message, clear_input\n", "    \n", "    except Exception as e:\n", "        # Return error message\n", "        error_message = message(f\"Sorry, an error occurred: {str(e)}\", \"assistant\")\n", "        return user_message, error_message\n", "\n", "# Start the server\n", "serve()\n", "''')\n", "```\n", "\n", "## Step 2: Create a Requirements File\n", "\n", "Let's create a requirements.txt file with all the necessary dependencies:\n", "\n", "```python\n", "# Create requirements.txt\n", "with open('laser_assistant/requirements.txt', 'w') as f:\n", "    f.write('''\n", "python-fasthtml\n", "MonsterUI\n", "llama-index-core\n", "llama-index-readers-file\n", "llama-index-embeddings-openai\n", "llama-index-llms-openai\n", "llama_index.postprocessor.cohere_rerank\n", "llama-index-program-openai\n", "qdrant-client\n", "llama-index-vector-stores-qdrant\n", "''')\n", "```\n", "\n", "## Step 3: Copy Necessary Data Files\n", "\n", "We need to copy the data files from your static directory to the deployment directory:\n", "\n", "```python\n", "# Copy necessary data files\n", "import shutil\n", "\n", "# Copy database file\n", "shutil.copy('static/laser_nodes.db', 'laser_assistant/laser_nodes.db')\n", "\n", "# Copy Qdrant pickle file\n", "shutil.copy('static/laser_qdrant.pkl', 'laser_assistant/laser_qdrant.pkl')\n", "```\n", "\n", "## Step 4: Create a plash.env File for Environment Variables\n", "\n", "Let's create a plash.env file to store the required API keys:\n", "\n", "```python\n", "# Create plash.env file\n", "with open('laser_assistant/plash.env', 'w') as f:\n", "    f.write('''\n", "export OPENAI_API_KEY=your_openai_api_key\n", "export COHERE_API_KEY=your_cohere_api_key\n", "''')\n", "```\n", "\n", "Note: You'll need to replace the placeholder values with your actual API keys before deploying.\n", "\n", "## Step 5: Deploy to Plash\n", "\n", "Now, let's use the plash_deploy command to deploy your application:\n", "\n", "```python\n", "# Change to the project directory\n", "import os\n", "os.chdir('laser_assistant')\n", "\n", "# Deploy to Plash\n", "!plash_deploy\n", "```\n", "\n", "The plash_deploy command will:\n", "1. Upload your code and data files to Plash\n", "2. Install the required dependencies\n", "3. Start your application on Plash's infrastructure\n", "4. Provide you with a URL where your application is accessible\n", "\n", "## Troubleshooting\n", "\n", "If you encounter any issues during deployment, here are some common troubleshooting steps:\n", "\n", "1. Make sure your PLASH_TOKEN and PLASH_EMAIL environment variables are correctly set\n", "2. Verify that all required files are included in your project directory\n", "3. Check that your requirements.txt includes all necessary dependencies\n", "4. Ensure that your plash.env file contains the correct API keys\n", "\n", "The deployment process might take a few minutes to complete. Once it's done, you should receive a URL where your application is accessible.\n", "\n", "Would you like me to proceed with these steps to deploy your application to Plash?"]}, {"cell_type": "code", "metadata": {}, "source": ["# Add custom CSS to the headers\n", "custom_css = Style(\"\"\"\n", "    body {\n", "        background-color: #f9fafb;\n", "    }\n", "    \n", "    #chat-messages {\n", "        min-height: 300px;\n", "    }\n", "    \n", "    .message-assistant pre {\n", "        background-color: #f8f9fa;\n", "        padding: 1rem;\n", "        border-radius: 0.5rem;\n", "        overflow-x: auto;\n", "    }\n", "    \n", "    #user-input:focus {\n", "        box-shadow: none;\n", "        border-color: #3b82f6;\n", "    }\n", "    \n", "    /* Improve the typography */\n", "    p {\n", "        line-height: 1.6;\n", "    }\n", "    \n", "    /* Markdown styling */\n", "    .markdown h1, .markdown h2, .markdown h3 {\n", "        margin-top: 1.5rem;\n", "        margin-bottom: 1rem;\n", "    }\n", "    \n", "    .markdown ul, .markdown ol {\n", "        padding-left: 1.5rem;\n", "        margin-bottom: 1rem;\n", "    }\n", "    \n", "    .markdown ul li, .markdown ol li {\n", "        margin-bottom: 0.5rem;\n", "    }\n", "    \n", "    .markdown code {\n", "        background-color: #f1f5f9;\n", "        padding: 0.2rem 0.4rem;\n", "        border-radius: 0.25rem;\n", "        font-size: 0.875rem;\n", "    }\n", "\"\"\")\n", "\n", "# Update the FastHTML app to include the custom CSS\n", "html_app, rt = fast_app(hdrs=(Theme.blue.headers(), custom_css, MarkdownJS()))"], "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "python3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 4}